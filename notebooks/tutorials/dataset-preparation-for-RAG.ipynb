{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d494287",
   "metadata": {},
   "source": [
    "# Dataset Preparation for RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "This notebook provides a complete, end-to-end workflow for preparing a dataset of documents for a **RAG (Retrieval-Augmented Generation)** system. We'll cover:\n",
    "\n",
    "1. **Document Conversion** with [Docling](https://docling-project.github.io/docling/) - converting PDFs to structured formats\n",
    "2. **RAG-Optimized Chunking** - breaking documents into semantically meaningful chunks\n",
    "3. **Embedding Generation** - using Snowflake Arctic embedding model\n",
    "4. **Vector Storage** - storing embeddings in Milvus vectordb for efficient retrieval\n",
    "\n",
    "## Why RAG-Specific Preparation Matters\n",
    "\n",
    "RAG systems require carefully prepared data to ensure:\n",
    "- **Contextual Completeness**: Each chunk contains sufficient context to be understood independently\n",
    "- **Semantic Coherence**: Chunks align with natural topic boundaries\n",
    "- **Optimal Size**: Balanced between detail (larger chunks) and precision (smaller chunks)\n",
    "- **Metadata Preservation**: Structural information (headings, tables, lists) is retained for better retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2064a85",
   "metadata": {},
   "source": [
    "## 📦 Installation\n",
    "\n",
    "Install the required packages. This may take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq docling sentence-transformers numpy jupyterlab \"pymilvus[milvus_lite]==2.6.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f0c69",
   "metadata": {},
   "source": [
    "## ⚠️ Important Notes\n",
    "\n",
    "**Exception Handling**: This notebook demonstrates the core workflow with minimal error handling for clarity. When using your own data or deploying to production:\n",
    "\n",
    "- Add try-except blocks around file I/O operations\n",
    "- Handle network errors for URL-based document loading\n",
    "- Validate document formats and sizes before processing\n",
    "- Implement timeouts for long-running operations\n",
    "- Add proper logging for debugging and monitoring\n",
    "- Handle cases where documents fail to convert or chunk\n",
    "- Validate embedding generation and database insertion success\n",
    "\n",
    "Example of adding exception handling:\n",
    "```python\n",
    "try:\n",
    "    result = converter.convert(file_path)\n",
    "    document = result.document\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to convert {file_path}: {str(e)}\")\n",
    "    continue  # Skip to next document\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51bd3a",
   "metadata": {},
   "source": [
    "## 🔧 Configuration\n",
    "\n",
    "### Set Input Documents\n",
    "\n",
    "Define the documents to process. You can mix URLs and local file paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dfd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "def get_safe_filename(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract a safe filename from a path or URL.\n",
    "    \n",
    "    Examples:\n",
    "        >>> get_safe_filename(\"https://example.com/docs/report.pdf\")\n",
    "        'report'\n",
    "        >>> get_safe_filename(\"https://drive.google.com/file/d/123/view\")\n",
    "        'doc_a3f8e91b2c'  # stable hash fallback\n",
    "        >>> get_safe_filename(\"/path/to/my document.pdf\")\n",
    "        'my_document'\n",
    "    \"\"\"\n",
    "    if file_path.startswith(('http://', 'https://')):\n",
    "        parsed = urlparse(file_path)\n",
    "        filename = Path(parsed.path.rstrip('/')).stem\n",
    "        # Treat generic basenames as non-descriptive and use stable hash fallback\n",
    "        UNHELPFUL = {\"view\", \"download\", \"file\", \"index\", \"blob\", \"raw\"}\n",
    "        if (not filename) or (filename.lower() in UNHELPFUL) or (len(filename) < 3):\n",
    "            from hashlib import md5\n",
    "            digest = md5(file_path.encode(\"utf-8\")).hexdigest()[:10]\n",
    "            filename = f\"doc_{digest}\"\n",
    "    else:\n",
    "        filename = Path(file_path).stem\n",
    "    \n",
    "    # Sanitize any remaining special characters\n",
    "    return re.sub(r'[^A-Za-z0-9._-]', '_', filename)\n",
    "\n",
    "print(\"✓ Helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f1a71",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Documents to process - replace with your own\n",
    "# Using sample PDFs with distinct names to avoid file naming collisions\n",
    "input_files = [\n",
    "    \"https://github.com/docling-project/docling/raw/v2.43.0/tests/data/pdf/2203.01017v2.pdf\",\n",
    "    \"https://github.com/docling-project/docling/raw/v2.43.0/tests/data/pdf/2206.01062.pdf\",\n",
    "    \"https://github.com/docling-project/docling/raw/v2.43.0/tests/data/pdf/2305.03393v1.pdf\",\n",
    "]\n",
    "\n",
    "# Output directory for intermediate files\n",
    "output_dir = Path(\"dataset-preparation-for-RAG/output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Milvus database configuration\n",
    "milvus_db_name = \"milvus_rag_demo.db\"\n",
    "milvus_db_path = output_dir / milvus_db_name\n",
    "\n",
    "print(f\"✓ Will process {len(input_files)} documents\")\n",
    "print(f\"✓ Output directory: {output_dir.resolve()}\")\n",
    "print(f\"✓ Milvus database: {milvus_db_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b1799",
   "metadata": {},
   "source": [
    "## 📄 Step 1: Document Conversion with Docling\n",
    "\n",
    "### RAG-Optimized Conversion Configuration\n",
    "\n",
    "For RAG systems, we configure Docling with specific settings:\n",
    "\n",
    "- **Table Structure Detection**: Essential for preserving tabular data relationships\n",
    "- **OCR**: Enables text extraction from scanned documents and images\n",
    "  - **Note**: OCR is enabled by default in this notebook for maximum text extraction\n",
    "  - Set `pipeline_options.do_ocr = False` below to disable if not needed\n",
    "  - **Requires Tesseract binary**: Please refer to the Docling [installation](https://docling-project.github.io/docling/installation/) docs if you're not running this notebook from a RHEL-based Workbench image that has it installed already\n",
    "- **Image Generation**: Useful for multimodal RAG or image-based context\n",
    "- **Enrichments** (optional): Code, formulas, picture descriptions add semantic richness\n",
    "\n",
    "These settings ensure we extract maximum structural and semantic information from documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2346773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractOcrOptions\n",
    "from docling.backend.docling_parse_v4_backend import DoclingParseV4DocumentBackend\n",
    "\n",
    "# Configure RAG-optimized pipeline\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "\n",
    "# Essential settings for RAG\n",
    "pipeline_options.do_table_structure = True  # Preserve table structure\n",
    "pipeline_options.table_structure_options.do_cell_matching = True  # Accurate cell matching\n",
    "pipeline_options.generate_page_images = False  # Set to True if you need images for multimodal RAG\n",
    "pipeline_options.generate_picture_images = True  # Extract pictures from documents\n",
    "\n",
    "# OCR configuration - enabled by default for maximum text extraction\n",
    "# Note: OCR requires the Tesseract binary (should be pre-installed on RHEL-based Workbench images)\n",
    "# Set to False if OCR is not needed: pipeline_options.do_ocr = False\n",
    "pipeline_options.do_ocr = True\n",
    "# Uncomment below to force OCR on all pages (useful for scanned documents)\n",
    "# pipeline_options.ocr_options = TesseractOcrOptions(\n",
    "#     force_full_page_ocr=False  # Set to True to force OCR on all pages (useful for scanned documents)\n",
    "# )\n",
    "\n",
    "# Performance settings\n",
    "pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "    num_threads=4,\n",
    "    device=AcceleratorDevice.AUTO\n",
    ")\n",
    "\n",
    "# Optional enrichments - enable these for richer semantic information\n",
    "# Note: These add processing time but improve RAG quality for technical documents\n",
    "pipeline_options.do_code_enrichment = False  # Enable for code-heavy documents\n",
    "pipeline_options.do_formula_enrichment = False  # Enable for math/scientific papers\n",
    "pipeline_options.do_picture_description = False  # Enable for multimodal RAG\n",
    "pipeline_options.do_picture_classification = False  # Enable to classify image types\n",
    "\n",
    "print(\"✓ Pipeline configured for RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d9caa",
   "metadata": {},
   "source": [
    "### Run Document Conversion\n",
    "\n",
    "Convert all documents to Docling's structured format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from docling_core.types.doc import ImageRefMode\n",
    "\n",
    "# Create converter\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "            backend=DoclingParseV4DocumentBackend,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Store converted documents\n",
    "converted_documents = []\n",
    "\n",
    "print(\"Starting document conversion...\\n\")\n",
    "\n",
    "for file_path in input_files:\n",
    "    print(f\"Converting: {file_path}\")\n",
    "    \n",
    "    result = converter.convert(file_path)\n",
    "    document = result.document\n",
    "    \n",
    "    # Compute safe filename once for reuse\n",
    "    file_name = get_safe_filename(file_path)\n",
    "    \n",
    "    # Store the document object for later processing\n",
    "    converted_documents.append({\n",
    "        'source': file_path,\n",
    "        'file_name': file_name,\n",
    "        'document': document,\n",
    "        'result': result\n",
    "    })\n",
    "    \n",
    "    # Save JSON (structured format)\n",
    "    json_path = output_dir / f\"{file_name}.json\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(document.export_to_dict(), f, indent=2)\n",
    "    print(f\"  ✓ Saved JSON: {json_path}\")\n",
    "    \n",
    "    # Save Markdown (human-readable)\n",
    "    md_path = output_dir / f\"{file_name}.md\"\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))\n",
    "    print(f\"  ✓ Saved Markdown: {md_path}\")\n",
    "    \n",
    "    # Print confidence metrics\n",
    "    print(f\"  ✓ Confidence: {result.confidence.mean_grade.name} (score: {result.confidence.mean_score:.3f})\\n\")\n",
    "\n",
    "print(f\"✓ Conversion complete! Processed {len(converted_documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246ff42",
   "metadata": {},
   "source": [
    "## 🧩 Step 2: RAG-Optimized Chunking\n",
    "\n",
    "### Chunking Strategy for RAG\n",
    "\n",
    "Effective chunking is critical for RAG systems. We'll implement **hybrid chunking** that:\n",
    "\n",
    "1. **Respects Document Structure**: Uses headings, sections, and natural boundaries\n",
    "2. **Maintains Context**: Includes metadata like titles, section names\n",
    "3. **Optimizes Chunk Size**: Tailored to embedding model (1024 dimensions for Snowflake Arctic)\n",
    "4. **Preserves Relationships**: Keeps tables, lists, and code blocks intact\n",
    "5. **Smart Merging**: Combines smaller chunks while respecting max token limits\n",
    "\n",
    "### Docling's Hybrid Chunker\n",
    "\n",
    "Docling provides a `HybridChunker` that intelligently splits documents while preserving structure and merging smaller chunks for optimal retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from docling_core.types.doc import DocItemLabel, DoclingDocument\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def chunk_document_for_rag(\n",
    "    doc: DoclingDocument,\n",
    "    source: str,\n",
    "    file_name: str,\n",
    "    max_tokens: int = 1024,\n",
    "    merge_peers: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Chunk a Docling document using hybrid chunking optimized for RAG.\n",
    "    \n",
    "    Args:\n",
    "        doc: DoclingDocument to chunk\n",
    "        source: Source file path/URL (for metadata/traceability)\n",
    "        file_name: Safe filename for generating chunk IDs\n",
    "        max_tokens: Maximum tokens per chunk (optimized for Snowflake Arctic: 1024)\n",
    "        merge_peers: Whether to merge smaller chunks at the same level\n",
    "    \n",
    "    Returns:\n",
    "        List of chunks with text and metadata\n",
    "    \"\"\"\n",
    "    # Initialize hybrid chunker with RAG-optimized parameters\n",
    "    # HybridChunker combines hierarchical structure with smart merging\n",
    "    chunker = HybridChunker(\n",
    "        max_tokens=max_tokens,      # Matches Snowflake Arctic's optimal context window\n",
    "        merge_peers=merge_peers      # Merge small adjacent chunks for better context\n",
    "    )\n",
    "    \n",
    "    # Perform chunking\n",
    "    chunk_iter = chunker.chunk(doc)\n",
    "    \n",
    "    # Process chunks and add metadata\n",
    "    chunks = []\n",
    "    for idx, chunk in enumerate(chunk_iter):\n",
    "        chunk_data = {\n",
    "            'chunk_id': f\"{file_name}_chunk_{idx}\",\n",
    "            'text': chunk.text,\n",
    "            'source': source,\n",
    "            'chunk_index': idx,\n",
    "            'metadata': {\n",
    "                'doc_items': [str(item.label) for item in chunk.meta.doc_items] if hasattr(chunk.meta, 'doc_items') else [],\n",
    "                'headings': chunk.meta.headings if hasattr(chunk.meta, 'headings') else [],\n",
    "                'token_count': len(chunk.text.split())  # Rough estimate\n",
    "            }\n",
    "        }\n",
    "        chunks.append(chunk_data)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"✓ Chunking function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4fc2c",
   "metadata": {},
   "source": [
    "### Apply Chunking to All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "\n",
    "print(\"Starting chunking process...\\n\")\n",
    "\n",
    "for doc_data in converted_documents:\n",
    "    source = doc_data['source']\n",
    "    file_name = doc_data['file_name']\n",
    "    document = doc_data['document']\n",
    "    \n",
    "    print(f\"Chunking: {source}\")\n",
    "    \n",
    "    chunks = chunk_document_for_rag(\n",
    "        doc=document,\n",
    "        source=source,\n",
    "        file_name=file_name,\n",
    "        max_tokens=1024,  # Optimized for Snowflake Arctic embeddings (1024 dims)\n",
    "        merge_peers=True   # Enable smart merging of smaller chunks\n",
    "    )\n",
    "    \n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"  ✓ Created {len(chunks)} chunks\\n\")\n",
    "\n",
    "print(f\"✓ Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Display sample chunk\n",
    "if all_chunks:\n",
    "    print(\"\\n=== Sample Chunk ===\")\n",
    "    sample = all_chunks[0]\n",
    "    print(f\"Chunk ID: {sample['chunk_id']}\")\n",
    "    print(f\"Source: {sample['source']}\")\n",
    "    print(f\"Text preview: {sample['text'][:200]}...\")\n",
    "    print(f\"Metadata: {sample['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b7753",
   "metadata": {},
   "source": [
    "### Save Chunks to Disk\n",
    "\n",
    "Save chunks as JSON for inspection and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1da01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_file = output_dir / \"rag_chunks.json\"\n",
    "with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Chunks saved to: {chunks_file.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867a01f",
   "metadata": {},
   "source": [
    "## 🔢 Step 3: Generate Embeddings with Snowflake Arctic\n",
    "\n",
    "### About Snowflake Arctic Embeddings\n",
    "\n",
    "[Snowflake Arctic Embed](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) is a family of text embedding models optimized for retrieval tasks. We'll use the **arctic-embed-l** variant which offers:\n",
    "\n",
    "- High-quality dense embeddings (1024 dimensions)\n",
    "- Excellent performance on retrieval benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load Snowflake Arctic embedding model\n",
    "print(\"Loading Snowflake Arctic embedding model...\")\n",
    "print(\"(This may take a few minutes on first run)\\n\")\n",
    "\n",
    "embedding_model = SentenceTransformer(\n",
    "    'Snowflake/snowflake-arctic-embed-l',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully\")\n",
    "print(f\"  Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f167e",
   "metadata": {},
   "source": [
    "### Generate Embeddings for All Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b103c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks: List[Dict[str, Any]], model: SentenceTransformer) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for all chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        model: SentenceTransformer model\n",
    "    \n",
    "    Returns:\n",
    "        Chunks with added 'embedding' field\n",
    "    \"\"\"\n",
    "    print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Extract text from chunks\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    # Generate embeddings in batches for efficiency\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    # Add embeddings to chunks\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        chunk['embedding'] = embedding.tolist()  # Convert to list for JSON serialization\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Generate embeddings\n",
    "all_chunks = generate_embeddings(all_chunks, embedding_model)\n",
    "\n",
    "print(f\"\\n✓ Embeddings generated for all {len(all_chunks)} chunks\")\n",
    "print(f\"Embedding dimension: {len(all_chunks[0]['embedding'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b76183",
   "metadata": {},
   "source": [
    "## 💾 Step 4: Store in Milvus Vector Database\n",
    "\n",
    "### About Milvus\n",
    "\n",
    "[Milvus](https://milvus.io/) is an open-source vector database built for AI applications. It provides:\n",
    "- Fast similarity search\n",
    "- Scalable vector storage\n",
    "- Multiple indexing algorithms\n",
    "- Hybrid search capabilities\n",
    "\n",
    "### Using Milvus Lite\n",
    "\n",
    "We're using **[Milvus Lite](https://milvus.io/docs/milvus_lite.md)** - a lightweight, file-based version of Milvus that:\n",
    "- Requires no Docker or server setup\n",
    "- Stores data locally in a file\n",
    "- Perfect for development, testing, and small-scale deployments\n",
    "- Uses the same API as full Milvus\n",
    "\n",
    "For production deployments with larger datasets, consider using the full Milvus server with Docker or Kubernetes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5882881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (\n",
    "    connections,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    "    utility\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "COLLECTION_NAME = \"rag_documents\"\n",
    "EMBEDDING_DIM = len(all_chunks[0]['embedding'])\n",
    "\n",
    "# Connect to Milvus Lite (file-based, no Docker needed!)\n",
    "print(\"Connecting to Milvus Lite (local file-based database)...\")\n",
    "print(\"This lightweight option is perfect for development and testing.\\n\")\n",
    "\n",
    "connections.connect(\n",
    "    alias=\"default\",\n",
    "    uri=str(milvus_db_path)  # Use configured database path\n",
    ")\n",
    "\n",
    "print(\"✓ Connected to Milvus Lite successfully!\")\n",
    "print(f\"✓ Database file: {milvus_db_path.resolve()}\")\n",
    "print(f\"✓ Collection name: '{COLLECTION_NAME}'\")\n",
    "print(f\"✓ Embedding dimension: {EMBEDDING_DIM}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c4d3f",
   "metadata": {},
   "source": [
    "### Create Collection Schema\n",
    "\n",
    "Define the schema for our RAG collection with fields for:\n",
    "- Chunk ID (primary key)\n",
    "- Text content\n",
    "- Source document\n",
    "- Metadata\n",
    "- Embedding vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collection schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=1024),\n",
    "    FieldSchema(name=\"chunk_index\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(\n",
    "    fields=fields,\n",
    "    description=\"RAG document chunks with embeddings\"\n",
    ")\n",
    "\n",
    "print(f\"Collection schema defined with {len(fields)} fields\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd40432",
   "metadata": {},
   "source": [
    "### Create or Recreate Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop existing collection if it exists\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    print(f\"Collection '{COLLECTION_NAME}' exists. Dropping...\")\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "    print(\"  ✓ Collection dropped\")\n",
    "\n",
    "# Create new collection\n",
    "print(f\"Creating collection '{COLLECTION_NAME}'...\")\n",
    "collection = Collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    using='default'\n",
    ")\n",
    "\n",
    "print(f\"✓ Collection '{COLLECTION_NAME}' created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd84af",
   "metadata": {},
   "source": [
    "### Insert Data into Milvus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40937585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for insertion\n",
    "print(f\"Preparing {len(all_chunks)} chunks for insertion...\")\n",
    "\n",
    "insert_data = [\n",
    "    [chunk['chunk_id'] for chunk in all_chunks],  # id\n",
    "    [chunk['text'][:65535] for chunk in all_chunks],  # text (truncate if needed)\n",
    "    [chunk['source'] for chunk in all_chunks],  # source\n",
    "    [chunk['chunk_index'] for chunk in all_chunks],  # chunk_index\n",
    "    [chunk['embedding'] for chunk in all_chunks],  # embedding\n",
    "]\n",
    "\n",
    "# Insert data\n",
    "print(\"Inserting data into Milvus...\")\n",
    "collection.insert(insert_data)\n",
    "\n",
    "print(f\"✓ Inserted {len(all_chunks)} chunks into Milvus\")\n",
    "\n",
    "# Flush to ensure data is persisted\n",
    "collection.flush()\n",
    "print(\"✓ Data flushed to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9577b8e",
   "metadata": {},
   "source": [
    "### Create Index and Load Collection\n",
    "\n",
    "Create an index on the embedding field to enable search functionality. For small datasets like this demo, a simple **FLAT** index works well. For production with larger datasets (1000+ vectors), consider **IVF_FLAT** or **HNSW** for better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c05b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define index parameters\n",
    "# For small datasets (<1000 vectors), FLAT index is simple and fast\n",
    "# For larger datasets, consider IVF_FLAT or HNSW\n",
    "index_params = {\n",
    "    \"metric_type\": \"COSINE\",  # Cosine similarity (works without pre-normalization)\n",
    "    \"index_type\": \"FLAT\",  # Simple brute-force search, perfect for small datasets\n",
    "}\n",
    "\n",
    "# For larger datasets (1000+ vectors), use IVF_FLAT instead:\n",
    "# index_params = {\n",
    "#     \"metric_type\": \"COSINE\",\n",
    "#     \"index_type\": \"IVF_FLAT\",\n",
    "#     \"params\": {\"nlist\": 128}  # Number of clusters\n",
    "# }\n",
    "\n",
    "print(\"Creating index on embedding field...\")\n",
    "collection.create_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "print(\"✓ Index created successfully\")\n",
    "\n",
    "# Load collection into memory for search\n",
    "collection.load()\n",
    "print(\"✓ Collection loaded into memory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe33a4",
   "metadata": {},
   "source": [
    "## 🔍 Step 5: Test RAG Retrieval\n",
    "\n",
    "Let's test our RAG system by performing a similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search for similar chunks using semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of similar chunks with scores\n",
    "    \"\"\"\n",
    "    # Generate embedding for query\n",
    "    query_embedding = embedding_model.encode([query])[0].tolist()\n",
    "    \n",
    "    # Search parameters (FLAT index doesn't require nprobe parameter)\n",
    "    search_params = {\n",
    "        \"metric_type\": \"COSINE\",\n",
    "        \"params\": {}  # Empty params for FLAT index (would need nprobe for IVF_FLAT)\n",
    "    }\n",
    "    \n",
    "    # Perform search\n",
    "    results = collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=top_k,\n",
    "        output_fields=[\"id\", \"text\", \"source\", \"chunk_index\"]\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for hits in results:\n",
    "        for hit in hits:\n",
    "            formatted_results.append({\n",
    "                'chunk_id': hit.entity.get('id'),\n",
    "                'text': hit.entity.get('text'),\n",
    "                'source': hit.entity.get('source'),\n",
    "                'chunk_index': hit.entity.get('chunk_index'),\n",
    "                'similarity_score': hit.score\n",
    "            })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "print(\"✓ Search function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63fc4ee",
   "metadata": {},
   "source": [
    "### Perform Multiple Test Searches\n",
    "\n",
    "Test the RAG system with different queries targeting different documents and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries for each document\n",
    "test_queries = [\n",
    "    \"What are the main contributions and results presented in the paper?\",\n",
    "    \"What methodology or approach is used in this research?\",\n",
    "    \"What are the key findings and conclusions?\"\n",
    "]\n",
    "\n",
    "# Run each test query and display results\n",
    "for query_num, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🔍 TEST QUERY {query_num}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    \n",
    "    # Perform search\n",
    "    search_results = search_similar_chunks(query, top_k=1)\n",
    "    \n",
    "    # Display results\n",
    "    for idx, result in enumerate(search_results, 1):\n",
    "        print(f\"\\n📄 Result {idx}\")\n",
    "        print(f\"   Similarity Score: {result['similarity_score']:.4f}\")\n",
    "        print(f\"   Source: {result['source'].split('/')[-1][:60]}...\")\n",
    "        print(f\"   Chunk ID: {result['chunk_id']}\")\n",
    "        print(f\"\\n   Text Preview:\")\n",
    "        print(f\"   {result['text'][:1000].replace(chr(10), ' ')}...\")\n",
    "        print(f\"   {'-'*80}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✅ RAG System Tested Successfully!\")\n",
    "print(f\"✓ Tested {len(test_queries)} queries across different documents\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827c33f",
   "metadata": {},
   "source": [
    "## 📚 Resources\n",
    "\n",
    "### Documentation\n",
    "- [Docling Documentation](https://docling-project.github.io/docling/)\n",
    "- [Milvus Documentation](https://milvus.io/docs)\n",
    "- [Snowflake Arctic Embed](https://huggingface.co/Snowflake/snowflake-arctic-embed-l)\n",
    "- [RAG Best Practices](https://docs.llamaindex.ai/en/stable/optimizing/production_rag/)\n",
    "\n",
    "### Related Examples & Tutorials\n",
    "- **RAG with Llama Stack**: For a complete RAG application implementation using Llama Stack, check out the [Docling RAG Notebook](https://github.com/opendatahub-io/rag/blob/main/demos/kfp/docling/pdf-conversion/docling_rag.ipynb) in the Open Data Hub RAG repository\n",
    "- **Data Processing Examples**: For additional example notebooks related to Data Processing, check the [Open Data Hub Data Processing](https://github.com/opendatahub-io/odh-data-processing/) repository on GitHub\n",
    "\n",
    "## 💬 Feedback\n",
    "\n",
    "We'd love to hear your feedback! Please [open an issue](https://github.com/opendatahub-io/odh-data-processing/issues) if you have suggestions or run into any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f386c",
   "metadata": {},
   "source": [
    "## 🧹 Cleanup (Optional)\n",
    "\n",
    "Run this cell to clean up resources if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to drop the collection\n",
    "# collection.drop()\n",
    "# print(\"✓ Collection dropped\")\n",
    "\n",
    "# # Disconnect from Milvus\n",
    "# connections.disconnect(\"default\")\n",
    "# print(\"✓ Disconnected from Milvus\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
